{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a script used to scrape the Times of Malta searching functionality. Unfortunately the site very recently (June 2019) altered it's Search page, so certain aspects of this code may be out of date.\n",
    "\n",
    "In this script we scrape the search page for the topic of Brexit, obtain a search ID using a cookie, set date parameters, and use this to traverse all result pages and access the URLs. The article data is stored within a CSV file called news5.csv. This data is then preprocessed using R 3.5 software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelfilletti/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import urllib\n",
    "import http.cookiejar, urllib.request\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we obtain information on the search page of Brexit\n",
    "#Since the search parameters are not included in the URL but are in the HTML\n",
    "#We must make use of the URLlib library\n",
    "\n",
    "#URL for search function of Times of Malta\n",
    "url = \"https://www.timesofmalta.com/search/\"\n",
    "\n",
    "#Construction of a search query by creating a header and parameters for search form\n",
    "#keywords - brexit\n",
    "#date - 1/1/2016 to 20/4/2019\n",
    "parameters = urllib.parse.urlencode([('data[Search][keywords]','brexit'),('data[Search][from_date]','2016-01-01'),('data[Search][to_date]','2019-04-20')]).encode(\"utf-8\")\n",
    "\n",
    "#header parameter for the request\n",
    "hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "\n",
    "#URL Request with the assigned headers and parameters\n",
    "req = urllib.request.Request(url,headers=hdr,data=parameters)\n",
    "\n",
    "#cookie instance\n",
    "cj = http.cookiejar.CookieJar()\n",
    "\n",
    "#Opening and reading the URL for the search with the parameters we set\n",
    "opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n",
    "response = opener.open(req)\n",
    "tom_page = response.read() #HTML of the search page\n",
    "\n",
    "#Here we have accessed the \"https://www.timesofmalta.com/search/\" page with the set parameters\n",
    "#However we still need to look at the different pages and obtain text etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a beautiful soup to parse the HTML\n",
    "soup1 = BeautifulSoup(tom_page, 'html.parser')\n",
    "\n",
    "#Returns the article title\n",
    "#articles = soup1.find_all('h2', {'class':'article_title'})\n",
    "\n",
    "#Returns the HTML text containing number of pages\n",
    "no_of_pages_txt = soup1.find_all('span', {'class':'pagination_pages'})\n",
    "\n",
    "#Clean text to obtain the number of pages (which is in the format Page 1 of XX)\n",
    "text = no_of_pages_txt[0].text.split('of')[1]\n",
    "\n",
    "#Convert string to integer\n",
    "no_of_pages = int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search ID -  searchId=5cfcc71c78d44e\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the Search ID\n",
    "#Wen carrying out a search on the ToM page, the parameters don't show up on the first page\n",
    "#This is necessary because when you go to the next page on ToM Search you are given a search ID\n",
    "page_button = soup1.find_all('a', {'class':'small_buttons'})\n",
    "button_txt =page_button[0].attrs['href']\n",
    "searchId = button_txt.split('/?')[1].split('&')\n",
    "print ('Search ID - ', searchId[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting articles from page  1\n",
      "getting articles from page  2\n",
      "getting articles from page  3\n",
      "getting articles from page  4\n",
      "getting articles from page  5\n",
      "getting articles from page  6\n",
      "getting articles from page  7\n",
      "getting articles from page  8\n",
      "getting articles from page  9\n",
      "getting articles from page  10\n",
      "getting articles from page  11\n",
      "getting articles from page  12\n",
      "getting articles from page  13\n",
      "getting articles from page  14\n",
      "getting articles from page  15\n",
      "getting articles from page  16\n",
      "getting articles from page  17\n",
      "getting articles from page  18\n",
      "getting articles from page  19\n",
      "getting articles from page  20\n",
      "getting articles from page  21\n",
      "getting articles from page  22\n",
      "getting articles from page  23\n",
      "getting articles from page  24\n",
      "getting articles from page  25\n",
      "getting articles from page  26\n",
      "getting articles from page  27\n",
      "getting articles from page  28\n",
      "getting articles from page  29\n",
      "getting articles from page  30\n",
      "getting articles from page  31\n",
      "getting articles from page  32\n",
      "getting articles from page  33\n",
      "getting articles from page  34\n",
      "getting articles from page  35\n",
      "getting articles from page  36\n",
      "getting articles from page  37\n",
      "getting articles from page  38\n",
      "getting articles from page  39\n",
      "getting articles from page  40\n",
      "getting articles from page  41\n",
      "getting articles from page  42\n",
      "getting articles from page  43\n",
      "getting articles from page  44\n",
      "getting articles from page  45\n",
      "getting articles from page  46\n",
      "getting articles from page  47\n",
      "getting articles from page  48\n",
      "getting articles from page  49\n",
      "getting articles from page  50\n",
      "getting articles from page  51\n",
      "getting articles from page  52\n",
      "getting articles from page  53\n",
      "getting articles from page  54\n",
      "getting articles from page  55\n",
      "getting articles from page  56\n",
      "getting articles from page  57\n",
      "getting articles from page  58\n",
      "getting articles from page  59\n",
      "getting articles from page  60\n",
      "getting articles from page  61\n",
      "getting articles from page  62\n",
      "getting articles from page  63\n",
      "getting articles from page  64\n",
      "getting articles from page  65\n",
      "getting articles from page  66\n",
      "getting articles from page  67\n",
      "getting articles from page  68\n",
      "getting articles from page  69\n",
      "getting articles from page  70\n",
      "getting articles from page  71\n",
      "getting articles from page  72\n",
      "getting articles from page  73\n",
      "getting articles from page  74\n",
      "getting articles from page  75\n",
      "getting articles from page  76\n",
      "getting articles from page  77\n",
      "getting articles from page  78\n",
      "getting articles from page  79\n",
      "getting articles from page  80\n",
      "getting articles from page  81\n",
      "getting articles from page  82\n",
      "getting articles from page  83\n",
      "getting articles from page  84\n",
      "getting articles from page  85\n",
      "getting articles from page  86\n",
      "getting articles from page  87\n",
      "getting articles from page  88\n",
      "getting articles from page  89\n",
      "getting articles from page  90\n",
      "getting articles from page  91\n",
      "getting articles from page  92\n",
      "getting articles from page  93\n",
      "getting articles from page  94\n",
      "getting articles from page  95\n",
      "getting articles from page  96\n",
      "getting articles from page  97\n",
      "getting articles from page  98\n",
      "getting articles from page  99\n",
      "getting articles from page  100\n",
      "getting articles from page  101\n",
      "getting articles from page  102\n",
      "getting articles from page  103\n",
      "getting articles from page  104\n",
      "getting articles from page  105\n",
      "getting articles from page  106\n",
      "getting articles from page  107\n",
      "getting articles from page  108\n",
      "getting articles from page  109\n",
      "getting articles from page  110\n",
      "getting articles from page  111\n",
      "getting articles from page  112\n",
      "getting articles from page  113\n",
      "getting articles from page  114\n",
      "getting articles from page  115\n",
      "getting articles from page  116\n",
      "getting articles from page  117\n",
      "getting articles from page  118\n",
      "getting articles from page  119\n",
      "getting articles from page  120\n",
      "getting articles from page  121\n",
      "getting articles from page  122\n",
      "getting articles from page  123\n",
      "getting articles from page  124\n",
      "getting articles from page  125\n",
      "getting articles from page  126\n",
      "getting articles from page  127\n",
      "getting articles from page  128\n",
      "getting articles from page  129\n",
      "getting articles from page  130\n",
      "getting articles from page  131\n",
      "getting articles from page  132\n",
      "getting articles from page  133\n",
      "getting articles from page  134\n",
      "getting articles from page  135\n",
      "getting articles from page  136\n",
      "getting articles from page  137\n",
      "getting articles from page  138\n",
      "getting articles from page  139\n",
      "getting articles from page  140\n",
      "getting articles from page  141\n",
      "getting articles from page  142\n",
      "getting articles from page  143\n",
      "getting articles from page  144\n",
      "getting articles from page  145\n",
      "getting articles from page  146\n",
      "getting articles from page  147\n",
      "getting articles from page  148\n",
      "getting articles from page  149\n",
      "getting articles from page  150\n",
      "getting articles from page  151\n"
     ]
    }
   ],
   "source": [
    "#Loop through all returned pages, get article urls and input into an array\n",
    "\n",
    "article_list =[]\n",
    "for i in range  (1,no_of_pages):  \n",
    "    #Constructing the URL\n",
    "    url = 'https://www.timesofmalta.com/search/index/?' + searchId[0] +'&page=' +str(i)\n",
    "    \n",
    "    #Obtaining the text on the page\n",
    "    req = urllib.request.Request(url,headers=hdr)\n",
    "    \n",
    "    #Open URL using same cookie otherwise Search ID does not work\n",
    "    opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n",
    "    response = opener.open(req)\n",
    "    search_page = response.read()\n",
    "    print('getting articles from page ',i) #progress counter\n",
    "    soup = BeautifulSoup(search_page, 'html.parser')\n",
    "    \n",
    "    #loop through each class element and get url of each article\n",
    "    for link in soup.select('h2.article_title > a:nth-of-type(1)'):\n",
    "        article_list.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commented out some variables that were not useful to us\n",
    "#Loop through each url and get article details\n",
    "title = []\n",
    "thearticle = []\n",
    "urllist = []\n",
    "#datel=[]\n",
    "#year=[]\n",
    "\n",
    "#Replace with article in article_list to scrape all articles\n",
    "for article in article_list:    \n",
    "    \n",
    "    paragraphtext = []\n",
    "    \n",
    "    #Set URL\n",
    "    url = 'http://www.timesofmalta.com'+article\n",
    "    \n",
    "    #Obtaining HTML text\n",
    "    req = urllib.request.Request(url,headers=hdr)\n",
    "    \n",
    "    #Open url using same cookie otherswise search id will not work\n",
    "    opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n",
    "    response = opener.open(req)\n",
    "    article_page = response.read()\n",
    "    \n",
    "    #Parsing HTML with BS\n",
    "    soup = BeautifulSoup(article_page, 'html.parser')      \n",
    "    \n",
    "    #get title\n",
    "    atitle = soup.find(class_='article_title')\n",
    "    thetitle = atitle.get_text()\n",
    "    \n",
    "    #Date\n",
    "    #date = soup.find(class_='article_date')\n",
    "    #articledate  = date.text.split(',')[1].strip()\n",
    "    #articleyear  = date.text.split(',')[2].strip()\n",
    "    \n",
    "    # get main article page\n",
    "    #articlebody = soup.find(class_='article_content')\n",
    "    \n",
    "    #Article text\n",
    "    articletext = soup.find_all('p')\n",
    "    #for thing in soup.find_all('p'): \n",
    "        #[re.sub('(\\n|\\r)', '', e) for e in thing.contents]\n",
    "    \n",
    "    #Cleaning the text\n",
    "    for paragraph in articletext:\n",
    "        text = paragraph.get_text()  \n",
    "        text = text.replace(',','')\n",
    "        text = text.replace(',','\"')\n",
    "        paragraphtext.append(text)    \n",
    "    \n",
    "    #Inputting all data into arrays\n",
    "    thearticle.append(paragraphtext)\n",
    "    #datel.append(articledate)\n",
    "    #year.append(articleyear)\n",
    "    title.append(thetitle)\n",
    "    urllist.append(url)\n",
    "\n",
    "    #Setting a dictionary with the data\n",
    "data = {'Title':title,  \n",
    "        'Article':thearticle,\n",
    "        'Article_URL':urllist}\n",
    "        #'Article_Date':datel,\n",
    "        #'Article_Year':year}\n",
    "\n",
    "#Inputting data to a dataframe and saving to excel\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "#Inputting the data to the csv\n",
    "#data.to_csv(r\"ToMDataTest.csv\", index=False)\n",
    "\n",
    "\n",
    "#writer = pd.ExcelWriter(r\"ToMData.xlsx\")\n",
    "#data.to_excel(writer)\n",
    "#writer.save()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
